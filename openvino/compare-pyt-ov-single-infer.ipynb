{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to compare PyTorch and OpenVINO output\n",
    "Following are the steps:\n",
    "\n",
    "1. Import packages and setup paths\n",
    "1. Get Mask\n",
    "1. Load PyTorch Model\n",
    "1. Run PyTorch Inference\n",
    "1. Convert PyTorch model to OpenVINO model\n",
    "1. Load OpenVINO Model\n",
    "1. Run OpenVINO Inference\n",
    "1. Plot Outputs and compare outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from BrainMaGe.models.networks import fetch_model\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from compare_utils import (\n",
    "    postprocess_prediction,\n",
    "    postprocess_save_output,\n",
    "    dice,\n",
    "    get_mask_image,\n",
    "    get_input_image\n",
    ")\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "brainmage_root = Path('../')\n",
    "\n",
    "input_path = '../compare_onnx/sub-A00028185_ses-NFB3_T1w_brain.nii.gz'\n",
    "mask_path= '../compare_onnx/sub-A00028185_ses-NFB3_T1w_brainmask.nii.gz'\n",
    "pt_output_path = 'pt-outfile' # PyTorch output file\n",
    "ov_output_path = 'ov-outfile' # ONNX output file\n",
    "\n",
    "pytorch_model_path = brainmage_root / 'BrainMaGe/weights/resunet_ma.pt'\n",
    "ov_model_dir = brainmage_root / 'BrainMaGe/weights/ov/fp32/'\n",
    "\n",
    "device=\"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_image Image size: (256, 256, 192) , dtype: float32 \n"
     ]
    }
   ],
   "source": [
    "mask_image = get_mask_image(mask_path)\n",
    "print(f\"mask_image Image size: {mask_image.shape} , dtype: {mask_image.dtype} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_model = fetch_model(modelname=\"resunet\", num_channels=1, num_classes=2, num_filters=16)\n",
    "checkpoint = torch.load(pytorch_model_path, map_location=torch.device('cpu'))\n",
    "pt_model.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run PyTorch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient_nib Image size: (256, 256, 192) \n",
      "Input Image size: torch.Size([1, 1, 128, 128, 128]) , dtype: torch.float32 \n",
      "PyTorch Inference time: 8.417 seconds\n",
      "Output Image size: (128, 128, 128) , dtype: float32 \n",
      "to_save Image size: (256, 256, 192) , dtype: float32 \n",
      "Output saved at:  pt-outfile\n"
     ]
    }
   ],
   "source": [
    "_ = pt_model.eval()\n",
    "\n",
    "input_image, patient_nib = get_input_image(input_path)\n",
    "print(f\"patient_nib Image size: {patient_nib.shape} \")\n",
    "print(f\"Input Image size: {input_image.shape} , dtype: {input_image.dtype} \")\n",
    "\n",
    "with torch.no_grad():\n",
    "    start = timer()\n",
    "    pt_output = pt_model(input_image)\n",
    "    end = timer()\n",
    "    print(f\"PyTorch Inference time: {end-start:.3f} seconds\")\n",
    "    \n",
    "    pt_output = pt_output.cpu().numpy()[0][0]\n",
    "    print(f\"Output Image size: {pt_output.shape} , dtype: {pt_output.dtype} \")\n",
    "    \n",
    "    pt_to_save, pt_save_nib = postprocess_save_output(pt_output, patient_nib, pt_output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch model Dice Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "pt_dice_score = dice(pt_to_save, mask_image)\n",
    "print(\"PyTorch model Dice Score: \", pt_dice_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch model Dice Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pt_dice_score = dice(pt_to_save, mask_image)\n",
    "print(\"PyTorch model Dice Score: \", pt_dice_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert PyTorch model to OpenVINO IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \tresunet(\n",
      "  (ins): in_conv(\n",
      "    (dropout): Dropout3d(p=0.3, inplace=False)\n",
      "    (in_0): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (in_1): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv0): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (conv1): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (conv2): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  )\n",
      "  (ds_0): DownsamplingModule(\n",
      "    (in_0): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv0): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "  )\n",
      "  (en_1): EncodingModule(\n",
      "    (dropout): Dropout3d(p=0.3, inplace=False)\n",
      "    (in_0): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (in_1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (conv1): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  )\n",
      "  (ds_1): DownsamplingModule(\n",
      "    (in_0): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "  )\n",
      "  (en_2): EncodingModule(\n",
      "    (dropout): Dropout3d(p=0.3, inplace=False)\n",
      "    (in_0): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (in_1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  )\n",
      "  (ds_2): DownsamplingModule(\n",
      "    (in_0): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "  )\n",
      "  (en_3): EncodingModule(\n",
      "    (dropout): Dropout3d(p=0.3, inplace=False)\n",
      "    (in_0): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (in_1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  )\n",
      "  (ds_3): DownsamplingModule(\n",
      "    (in_0): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "  )\n",
      "  (en_4): EncodingModule(\n",
      "    (dropout): Dropout3d(p=0.3, inplace=False)\n",
      "    (in_0): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (in_1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (conv1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  )\n",
      "  (us_3): UpsamplingModule(\n",
      "    (interpolate): Interpolate()\n",
      "    (conv0): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "  )\n",
      "  (de_3): DecodingModule(\n",
      "    (in_0): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (in_1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (in_2): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv0): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  )\n",
      "  (us_2): UpsamplingModule(\n",
      "    (interpolate): Interpolate()\n",
      "    (conv0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "  )\n",
      "  (de_2): DecodingModule(\n",
      "    (in_0): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (in_1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (in_2): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv0): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  )\n",
      "  (us_1): UpsamplingModule(\n",
      "    (interpolate): Interpolate()\n",
      "    (conv0): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "  )\n",
      "  (de_1): DecodingModule(\n",
      "    (in_0): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (in_1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (in_2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv0): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (conv1): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (conv2): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  )\n",
      "  (us_0): UpsamplingModule(\n",
      "    (interpolate): Interpolate()\n",
      "    (conv0): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "  )\n",
      "  (out): out_conv(\n",
      "    (in_0): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (in_1): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (in_2): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (in_3): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv0): Conv3d(32, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (conv1): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (conv2): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (conv3): Conv3d(16, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "  )\n",
      ")\n",
      "\t- Path for generated IR: \t/home/sdp/ravi/upenn/BrainMaGe/ravi/.\n",
      "\t- IR output name: \t../BrainMaGe/weights/ov/fp32/resunet_ma\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \t[1, 1, 128, 128, 128]\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tNone\n",
      "\t- Reverse input channels: \tFalse\n",
      "[ WARNING ]  Using fallback to produce IR.\n",
      "[ SUCCESS ] Generated IR version 10 model.\n",
      "[ SUCCESS ] XML file: /home/sdp/ravi/upenn/BrainMaGe/BrainMaGe/weights/ov/fp32/resunet_ma.xml\n",
      "[ SUCCESS ] BIN file: /home/sdp/ravi/upenn/BrainMaGe/BrainMaGe/weights/ov/fp32/resunet_ma.bin\n",
      "[ SUCCESS ] Total execution time: 19.01 seconds. \n",
      "[ SUCCESS ] Memory consumed: 9257 MB. \n",
      "Traceback (most recent call last):\n",
      "  File \"export-to-ov.py\", line 30, in <module>\n",
      "    net = ie.read_network('model.xml')\n",
      "  File \"ie_api.pyx\", line 324, in openvino.inference_engine.ie_api.IECore.read_network\n",
      "  File \"ie_api.pyx\", line 337, in openvino.inference_engine.ie_api.IECore.read_network\n",
      "Exception: Path to the model model.xml doesn't exist or it's a directory\n"
     ]
    }
   ],
   "source": [
    "!python export-to-ov.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../BrainMaGe/weights/ov/fp32\n",
      "total 32540\n",
      "-rw-rw-r-- 1 sdp sdp 33152004 Jul  8 14:28 resunet_ma.bin\n",
      "-rw-rw-r-- 1 sdp sdp    16759 Jul  8 14:28 resunet_ma.mapping\n",
      "-rw-rw-r-- 1 sdp sdp   145170 Jul  8 14:28 resunet_ma.xml\n"
     ]
    }
   ],
   "source": [
    "print(ov_model_dir)\n",
    "!ls -l $ov_model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load OpenVINO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_model_name = \"resunet_ma\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.inference_engine import IECore\n",
    "\n",
    "model_xml = f'{ov_model_dir}/{ov_model_name}.xml'\n",
    "model_bin = f'{ov_model_dir}/{ov_model_name}.bin'\n",
    "\n",
    "# Load network to the plugin\n",
    "ie = IECore()\n",
    "net = ie.read_network(model=model_xml, weights=model_bin)\n",
    "exec_net = ie.load_network(network=net, device_name=\"CPU\")\n",
    "del net\n",
    "\n",
    "input_layer = next(iter(exec_net.input_info))\n",
    "output_layer = next(iter(exec_net.outputs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run OpenVINO Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenVINO Inference time: 0.204 seconds\n"
     ]
    }
   ],
   "source": [
    "input_image, patient_nib = get_input_image(input_path)\n",
    "\n",
    "#Run the Inference on the Input image...\n",
    "start = timer()\n",
    "ov_output = exec_net.infer(inputs={input_layer: input_image})\n",
    "end = timer()\n",
    "print(f\"OpenVINO Inference time: {end-start:.3f} seconds\")\n",
    "\n",
    "ov_output = ov_output[output_layer][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ov_output size: (128, 128, 128) , dtype: float32 \n"
     ]
    }
   ],
   "source": [
    "print(f\"ov_output size: {ov_output.shape} , dtype: {ov_output.dtype} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_save Image size: (256, 256, 192) , dtype: float32 \n",
      "Output saved at:  ov-outfile\n"
     ]
    }
   ],
   "source": [
    "ov_to_save, ov_to_save_nib = postprocess_save_output(ov_output, patient_nib, ov_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OV Dice Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "ov_dice_score = dice(ov_to_save, mask_image)\n",
    "print(\"OV Dice Score: \", ov_dice_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dice vs ONNX Dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Dice Score:  1.0\n",
      "ONNX Dice Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch Dice Score: \", pt_dice_score)\n",
    "print(\"ONNX Dice Score: \", ov_dice_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "ax1.imshow(mask_image[:,:,100])\n",
    "ax1.set_title('Mask Image')\n",
    "\n",
    "ax2.imshow(pt_to_save[:,:,100])\n",
    "ax2.set_title('PyTorch Output')\n",
    "\n",
    "ax3.imshow(ov_to_save[:,:,100])\n",
    "_ = ax3.set_title('OV Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_counts: Counter({0.0: 12579475, 1.0: 3437})\n",
      "pt_counts: Counter({0.0: 12579475, 1.0: 3437})\n",
      "ort_counts: Counter({0.0: 12579475, 1.0: 3437})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "mask_counts = Counter(mask_image.flatten())\n",
    "pt_counts = Counter(pt_to_save.flatten())\n",
    "ov_counts = Counter(ov_to_save.flatten())\n",
    "print(\"mask_counts:\", mask_counts)\n",
    "print(\"pt_counts:\", pt_counts)\n",
    "print(\"ort_counts:\", ov_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
